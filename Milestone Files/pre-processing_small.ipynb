{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import tokenize, stem, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1013000 data points.\n",
      "There are 1013 classes.\n"
     ]
    }
   ],
   "source": [
    "# load dataset with reddit posts\n",
    "df = pd.read_csv('rspct.tsv', sep = '\\t')\n",
    "\n",
    "# return number of datapoints and number of classes\n",
    "print(f\"There are {df.shape[0]} data points.\")\n",
    "print(f\"There are {df['subreddit'].nunique()} classes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 25000 data points.\n",
      "There are 25 classes.\n"
     ]
    }
   ],
   "source": [
    "# select number of subrreddits to train on\n",
    "n = 25 # number was decided based on hardware constraints\n",
    "n_classes = df.subreddit.unique()[:n].tolist()\n",
    "\n",
    "# filter initial dataset to first n classes\n",
    "df = df[df['subreddit'].isin(n_classes)]\n",
    "\n",
    "# return number of datapoints and number of classes\n",
    "print(f\"There are {df.shape[0]} data points.\")\n",
    "print(f\"There are {df['subreddit'].nunique()} classes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 20000 data points in the training set\n",
      "There are 25 classes in the training set\n",
      "There are 5000 data points in the test set\n",
      "There are 25 classes in the test set\n"
     ]
    }
   ],
   "source": [
    "# seperate out training and testing sets\n",
    "df_train = df.groupby('subreddit').sample(frac=0.8)\n",
    "df_test = df[~df.isin(df_train)].dropna(how = 'all')\n",
    "\n",
    "# return number of data points and number of classes for test and training\n",
    "print(f\"There are {df_train.shape[0]} data points in the training set\")\n",
    "print(f\"There are {df_train['subreddit'].nunique()} classes in the training set\")\n",
    "\n",
    "print(f\"There are {df_test.shape[0]} data points in the test set\")\n",
    "print(f\"There are {df_test['subreddit'].nunique()} classes in the test set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing the text\n",
    "\n",
    "# remove all lowercase from reddit posts and titles\n",
    "df_train['selftext'] = df_train.selftext.apply(lambda x: x.lower())\n",
    "df_test['selftext'] = df_test.selftext.apply(lambda x: x.lower())\n",
    "df_train['title'] = df_train.title.apply(lambda x: x.lower())\n",
    "df_test['title'] = df_test.title.apply(lambda x: x.lower())\n",
    "\n",
    "# remove all html tags\n",
    "df_train['selftext'] = df_train['selftext'].replace(r'<.*?>', '', regex=True)\n",
    "df_test['selftext'] = df_test['selftext'].replace(r'<.*?>', '', regex=True)\n",
    "\n",
    "# remove all special characters, including punctuation\n",
    "df_train['selftext'] = df_train['selftext'].replace(r'[^\\w\\s]|_', '', regex=True)\n",
    "df_test['selftext'] = df_test['selftext'].replace(r'[^\\w\\s]|_', '', regex=True)\n",
    "df_train['title'] = df_train['title'].replace(r'[^\\w\\s]|_', '', regex=True)\n",
    "df_test['title'] = df_test['title'].replace(r'[^\\w\\s]|_', '', regex=True)\n",
    "\n",
    "# remove all URLs\n",
    "df_train['selftext'] = df_train['selftext'].replace(r'http\\S+|www.\\.\\S+', '', regex=True)\n",
    "df_test['selftext'] = df_test['selftext'].replace(r'http\\S+|www.\\.\\S+', '', regex=True)\n",
    "\n",
    "# strip all stop words (e.g. the, and, or)\n",
    "df_train['selftext'] = df_train['selftext'].replace(r'(\\s*\\b(?:a|an|and|are|as|at|be|but|by|for|if|in|into|is|it|no|not|of|on|or|such|that|the|their|then|there|these|they|this|to|was|will|with|my|oh|i|were|werent|was|wasnt|do|does))\\b', '', regex=True)\n",
    "df_test['selftext'] = df_test['selftext'].replace(r'(\\s*\\b(?:a|an|and|are|as|at|be|but|by|for|if|in|into|is|it|no|not|of|on|or|such|that|the|their|then|there|these|they|this|to|was|will|with|my|oh|i|were|werent|was|wasnt|do|does))\\b', '', regex=True)\n",
    "df_train['title'] = df_train['title'].replace(r'(\\s*\\b(?:a|an|and|are|as|at|be|but|by|for|if|in|into|is|it|no|not|of|on|or|such|that|the|their|then|there|these|they|this|to|was|will|with|my|oh|i|were|werent|was|wasnt|do|does))\\b', '', regex=True)\n",
    "df_test['title'] = df_test['title'].replace(r'(\\s*\\b(?:a|an|and|are|as|at|be|but|by|for|if|in|into|is|it|no|not|of|on|or|such|that|the|their|then|there|these|they|this|to|was|will|with|my|oh|i|were|werent|was|wasnt|do|does))\\b', '', regex=True)\n",
    "\n",
    "# remove usernames and words with digits\n",
    "df_train['selftext'] = df_train['selftext'].replace(r'\\s\\w*\\d+\\w*', '', regex=True)\n",
    "df_test['selftext'] = df_test['selftext'].replace(r'\\s\\w*\\d+\\w*', '', regex=True)\n",
    "df_train['title'] = df_train['title'].replace(r'\\s\\w*\\d+\\w*', '', regex=True)\n",
    "df_test['title'] = df_test['title'].replace(r'\\s\\w*\\d+\\w*', '', regex=True)\n",
    "\n",
    "# remove extra spaces\n",
    "df_train['selftext'] = df_train['selftext'].replace(r'\\s\\s+', ' ', regex=True)\n",
    "df_test['selftext'] = df_test['selftext'].replace(r'\\s\\s+', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Lemmatizer object\n",
    "lemmatizer = stem.WordNetLemmatizer()\n",
    "\n",
    "# Get the POS Tag for lemmatization (reference: https://medium.com/@yashj302/lemmatization-f134b3089429 )\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].lower()\n",
    "    tag_dict = {'j': corpus.wordnet.ADJ,\n",
    "                'n': corpus.wordnet.NOUN,\n",
    "                'v': corpus.wordnet.VERB,\n",
    "                'r': corpus.wordnet.ADV}\n",
    "    return tag_dict.get(tag, corpus.wordnet.NOUN)\n",
    "\n",
    "# apply lemmatizer on the text and title columns\n",
    "df_train['selftext'] = df_train.selftext.apply(lambda x: ' '.join([lemmatizer.lemmatize(y, get_wordnet_pos(y)) for y in tokenize.word_tokenize(x)]))\n",
    "df_test['selftext'] = df_test.selftext.apply(lambda x: ' '.join([lemmatizer.lemmatize(y, get_wordnet_pos(y)) for y in tokenize.word_tokenize(x)]))\n",
    "df_train['title'] = df_train.title.apply(lambda x: ' '.join([lemmatizer.lemmatize(y, get_wordnet_pos(y)) for y in tokenize.word_tokenize(x)]))\n",
    "df_test['title'] = df_test.title.apply(lambda x: ' '.join([lemmatizer.lemmatize(y, get_wordnet_pos(y)) for y in tokenize.word_tokenize(x)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results of lemmatization for future work\n",
    "df_train.to_csv(\"training_set_milestone.csv\")\n",
    "df_test.to_csv(\"test_set_milestone.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e97064513f0d268e2f073204e543b8077386c7433ee9f7232070081c2302a999"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
